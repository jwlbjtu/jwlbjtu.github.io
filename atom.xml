<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Code with Love</title>
  
  <subtitle>The road to learn</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jwlbjtu.github.io/"/>
  <updated>2018-03-16T03:40:40.000Z</updated>
  <id>https://jwlbjtu.github.io/</id>
  
  <author>
    <name>Wenlong Jiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Multiple Linear Regression by Scikit-Learn</title>
    <link href="https://jwlbjtu.github.io/2018/03/15/Multiple-Linear-Regression-by-Scikit-Learn/"/>
    <id>https://jwlbjtu.github.io/2018/03/15/Multiple-Linear-Regression-by-Scikit-Learn/</id>
    <published>2018-03-16T03:22:31.000Z</published>
    <updated>2018-03-16T03:40:40.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Multiple-Linear-Regression-Intuition"><a href="#Multiple-Linear-Regression-Intuition" class="headerlink" title="Multiple Linear Regression Intuition"></a>Multiple Linear Regression Intuition</h3><p><strong>Multiple Linear Regression</strong> presents linear relationship between mutiple independent variables and dependent variable. </p><p><strong>Formula</strong>: $$y = b_0 + b_1 <em> X_1 + … + b_n </em> X_n$$<br><a id="more"></a></p><h3 id="Multiple-Linear-Regression-Implementaion"><a href="#Multiple-Linear-Regression-Implementaion" class="headerlink" title="Multiple Linear Regression Implementaion"></a>Multiple Linear Regression Implementaion</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing the libaraies</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Importing the dataset</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">'50_Startups.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show dataset</span></span><br><span class="line">dataset</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th style="text-align:center">R&amp;D Spend</th><th style="text-align:center">Administration</th><th style="text-align:center">Marketing Spend</th><th style="text-align:right">State</th><th style="text-align:right">Profit</th></tr></thead><tbody><tr><td>0</td><td style="text-align:center">165349.20</td><td style="text-align:center">136897.80</td><td style="text-align:center">471784.10</td><td style="text-align:right">New York</td><td style="text-align:right">192261.83</td></tr><tr><td>1</td><td style="text-align:center">162597.70</td><td style="text-align:center">151377.59</td><td style="text-align:center">443898.53</td><td style="text-align:right">California</td><td style="text-align:right">191792.06</td></tr><tr><td>2</td><td style="text-align:center">153441.51</td><td style="text-align:center">101145.55</td><td style="text-align:center">407934.54</td><td style="text-align:right">Florida</td><td style="text-align:right">191050.39</td></tr><tr><td>3</td><td style="text-align:center">144372.41</td><td style="text-align:center">118671.85</td><td style="text-align:center">383199.62</td><td style="text-align:right">New York</td><td style="text-align:right">182901.99</td></tr><tr><td>4</td><td style="text-align:center">142107.34</td><td style="text-align:center">91391.77</td><td style="text-align:center">366168.42</td><td style="text-align:right">Florida</td><td style="text-align:right">166187.94</td></tr><tr><td>5</td><td style="text-align:center">131876.90</td><td style="text-align:center">99814.71</td><td style="text-align:center">362861.36</td><td style="text-align:right">New York</td><td style="text-align:right">156991.12</td></tr><tr><td>6</td><td style="text-align:center">134615.46</td><td style="text-align:center">147198.87</td><td style="text-align:center">127716.82</td><td style="text-align:right">California</td><td style="text-align:right">156122.51</td></tr><tr><td>7</td><td style="text-align:center">130298.13</td><td style="text-align:center">145530.06</td><td style="text-align:center">323876.68</td><td style="text-align:right">Florida</td><td style="text-align:right">155752.60</td></tr><tr><td>8</td><td style="text-align:center">120542.52</td><td style="text-align:center">148718.95</td><td style="text-align:center">311613.29</td><td style="text-align:right">New York</td><td style="text-align:right">152211.77</td></tr><tr><td>9</td><td style="text-align:center">123334.88</td><td style="text-align:center">108679.17</td><td style="text-align:center">304981.62</td><td style="text-align:right">California</td><td style="text-align:right">149759.96</td></tr></tbody></table><p>…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">4</span>].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show X</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[165349.2, 136897.8, 471784.1, &apos;New York&apos;],       [162597.7, 151377.59, 443898.53, &apos;California&apos;],       [153441.51, 101145.55, 407934.54, &apos;Florida&apos;],       [144372.41, 118671.85, 383199.62, &apos;New York&apos;],       [142107.34, 91391.77, 366168.42, &apos;Florida&apos;],       [131876.9, 99814.71, 362861.36, &apos;New York&apos;],       [134615.46, 147198.87, 127716.82, &apos;California&apos;],       [130298.13, 145530.06, 323876.68, &apos;Florida&apos;],       [120542.52, 148718.95, 311613.29, &apos;New York&apos;],       [123334.88, 108679.17, 304981.62, &apos;California&apos;],       ...], dtype=object)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show y</span></span><br><span class="line">y</span><br></pre></td></tr></table></figure><pre><code>array([ 192261.83,  191792.06,  191050.39,  182901.99,  166187.94,        156991.12,  156122.51,  155752.6 ,  152211.77,  149759.96,        ... ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Caegorical data encoding</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelEncoder_X = LabelEncoder()</span><br><span class="line">X[:, <span class="number">3</span>]= labelEncoder_X.fit_transform(X[:, <span class="number">3</span>])</span><br><span class="line">oneHotEncoder = OneHotEncoder(categorical_features = [<span class="number">3</span>])</span><br><span class="line">X = oneHotEncoder.fit_transform(X).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show X</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,          1.65349200e+05,   1.36897800e+05,   4.71784100e+05],       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,          1.62597700e+05,   1.51377590e+05,   4.43898530e+05],       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,          1.53441510e+05,   1.01145550e+05,   4.07934540e+05],       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,          1.44372410e+05,   1.18671850e+05,   3.83199620e+05],       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,          1.42107340e+05,   9.13917700e+04,   3.66168420e+05],       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,          1.31876900e+05,   9.98147100e+04,   3.62861360e+05],       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,          1.34615460e+05,   1.47198870e+05,   1.27716820e+05],       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,          1.30298130e+05,   1.45530060e+05,   3.23876680e+05],       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,          1.20542520e+05,   1.48718950e+05,   3.11613290e+05],       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,          1.23334880e+05,   1.08679170e+05,   3.04981620e+05],       ...])</code></pre><p>During the categorical data encoding, we created three dummy variables to present ‘New York’, ‘California’ and ‘Florida’. Because they have strong co-relationship between each other, to avoid dummy variable trap we need to remove one dummy variable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Avoiding the Dummy Variable Trap</span></span><br><span class="line">X = X[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show X</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[  0.00000000e+00,   1.00000000e+00,   1.65349200e+05,          1.36897800e+05,   4.71784100e+05],       [  0.00000000e+00,   0.00000000e+00,   1.62597700e+05,          1.51377590e+05,   4.43898530e+05],       [  1.00000000e+00,   0.00000000e+00,   1.53441510e+05,          1.01145550e+05,   4.07934540e+05],       [  0.00000000e+00,   1.00000000e+00,   1.44372410e+05,          1.18671850e+05,   3.83199620e+05],       [  1.00000000e+00,   0.00000000e+00,   1.42107340e+05,          9.13917700e+04,   3.66168420e+05],       [  0.00000000e+00,   1.00000000e+00,   1.31876900e+05,          9.98147100e+04,   3.62861360e+05],       [  0.00000000e+00,   0.00000000e+00,   1.34615460e+05,          1.47198870e+05,   1.27716820e+05],       [  1.00000000e+00,   0.00000000e+00,   1.30298130e+05,          1.45530060e+05,   3.23876680e+05],       [  0.00000000e+00,   1.00000000e+00,   1.20542520e+05,          1.48718950e+05,   3.11613290e+05],       [  0.00000000e+00,   0.00000000e+00,   1.23334880e+05,          1.08679170e+05,   3.04981620e+05],       ...])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split the dataset into Training set and Test set</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting Multiple Linear Regression to the Training set</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predicting the Test set</span></span><br><span class="line">y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show predict result</span></span><br><span class="line">y_pred</span><br></pre></td></tr></table></figure><pre><code>array([ 103015.20159795,  132582.27760817,  132447.73845176,         71976.09851257,  178537.48221058,  116161.24230165,         67851.69209675,   98791.73374686,  113969.43533013,        167921.06569553])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show Test set result</span></span><br><span class="line">y_test</span><br></pre></td></tr></table></figure><pre><code>array([ 103282.38,  144259.4 ,  146121.95,   77798.83,  191050.39,        105008.31,   81229.06,   97483.56,  110352.25,  166187.94])</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Multiple-Linear-Regression-Intuition&quot;&gt;&lt;a href=&quot;#Multiple-Linear-Regression-Intuition&quot; class=&quot;headerlink&quot; title=&quot;Multiple Linear Regression Intuition&quot;&gt;&lt;/a&gt;Multiple Linear Regression Intuition&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Multiple Linear Regression&lt;/strong&gt; presents linear relationship between mutiple independent variables and dependent variable. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Formula&lt;/strong&gt;: $$y = b_0 + b_1 &lt;em&gt; X_1 + … + b_n &lt;/em&gt; X_n$$&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://jwlbjtu.github.io/categories/Machine-Learning/"/>
    
      <category term="Regression" scheme="https://jwlbjtu.github.io/categories/Machine-Learning/Regression/"/>
    
    
      <category term="Machine Learning" scheme="https://jwlbjtu.github.io/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="https://jwlbjtu.github.io/tags/Python/"/>
    
      <category term="Scikit-Learn" scheme="https://jwlbjtu.github.io/tags/Scikit-Learn/"/>
    
  </entry>
  
  <entry>
    <title>Simple Linear Regression by Scikit-Learn</title>
    <link href="https://jwlbjtu.github.io/2018/03/14/Simple-Linear-Regression-by-Scikit-Learn/"/>
    <id>https://jwlbjtu.github.io/2018/03/14/Simple-Linear-Regression-by-Scikit-Learn/</id>
    <published>2018-03-14T22:29:31.000Z</published>
    <updated>2018-03-14T23:31:34.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Simple-Linear-Regression-Intuition"><a href="#Simple-Linear-Regression-Intuition" class="headerlink" title="Simple Linear Regression Intuition"></a>Simple Linear Regression Intuition</h3><p><strong>Simple linear regression</strong> is a statistical method that allows us to summarize and study relationships between two continuous variables:</p><ul><li>One variable denoted X, is regarded as independent vaiable.</li><li>The other variable, denoted y, is regarded as the dependent variable.</li></ul><p><strong>Formula</strong>: $$y = b_0 + b_1 * X_1$$<br><a id="more"></a></p><h3 id="Simple-Linear-Regression-Implimentation"><a href="#Simple-Linear-Regression-Implimentation" class="headerlink" title="Simple Linear Regression Implimentation"></a>Simple Linear Regression Implimentation</h3><p>Here we use a simple dataset to train a simple linear regression model. The dataset contains the year of working experences and the salary for each working years.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing the Libaraies</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Importing the Dataset</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">'Salary_Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">1</span>].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show X - The year of working experiences</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[  1.1],       [  1.3],       [  1.5],       [  2. ],       [  2.2],       [  2.9],       [  3. ],       [  3.2],       [  3.2],       [  3.7],       [  3.9],       [  4. ],       [  4. ],       [  4.1],       [  4.5],       [  4.9],       [  5.1],       [  5.3],       [  5.9],       [  6. ],       [  6.8],       [  7.1],       [  7.9],       [  8.2],       [  8.7],       [  9. ],       [  9.5],       [  9.6],       [ 10.3],       [ 10.5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show y - The salary reflected by the working experiences</span></span><br><span class="line">y</span><br></pre></td></tr></table></figure><pre><code>array([  39343.,   46205.,   37731.,   43525.,   39891.,   56642.,         60150.,   54445.,   64445.,   57189.,   63218.,   55794.,         56957.,   57081.,   61111.,   67938.,   66029.,   83088.,         81363.,   93940.,   91738.,   98273.,  101302.,  113812.,        109431.,  105582.,  116969.,  112635.,  122391.,  121872.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the dataset</span></span><br><span class="line">plt.scatter(X, y, color = <span class="string">'red'</span>)</span><br><span class="line">plt.title(<span class="string">'Salary vs Experience'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Years of Experience'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Salary'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_8_0.png" alt=""></p><p>From the diagram we can see that he dataset is following some kind of linear relationship between X and y. Our goal is to find a model that could fit the data perfectly.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splitting the dataset into Training set and Test set</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">1</span>/<span class="number">3</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting Simple Linear Regression to the Training set</span></span><br><span class="line"><span class="comment"># Scikit-Learn does the feature scaling for us so we do not need to do that</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict the Test set result</span></span><br><span class="line">y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print y_pred</span></span><br><span class="line">y_pred</span><br></pre></td></tr></table></figure><pre><code>array([  40835.10590871,  123079.39940819,   65134.55626083,         63265.36777221,  115602.64545369,  108125.8914992 ,        116537.23969801,   64199.96201652,   76349.68719258,        100649.1375447 ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compare with y_test</span></span><br><span class="line">y_test</span><br></pre></td></tr></table></figure><pre><code>array([  37731.,  122391.,   57081.,   63218.,  116969.,  109431.,        112635.,   55794.,   83088.,  101302.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Viualising the Training set result</span></span><br><span class="line">plt.scatter(X_train, y_train, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color = <span class="string">'blue'</span>)</span><br><span class="line">plt.title(<span class="string">'Salary vs Experience'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Years of Experience'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Salary'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_12_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Viualising the Test set result</span></span><br><span class="line">plt.scatter(X_test, y_test, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color = <span class="string">'blue'</span>)</span><br><span class="line">plt.title(<span class="string">'Salary vs Experience'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Years of Experience'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Salary'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt=""></p><p>By using Scikit-Learn LinearRegression module, we trained out model with X_train, y_train. And we found our Simple Linear Regression model showing as the blue line in above two diagrams. In the last diagram we can see our model fits the test dataset very well. </p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Simple-Linear-Regression-Intuition&quot;&gt;&lt;a href=&quot;#Simple-Linear-Regression-Intuition&quot; class=&quot;headerlink&quot; title=&quot;Simple Linear Regression Intuition&quot;&gt;&lt;/a&gt;Simple Linear Regression Intuition&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Simple linear regression&lt;/strong&gt; is a statistical method that allows us to summarize and study relationships between two continuous variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One variable denoted X, is regarded as independent vaiable.&lt;/li&gt;
&lt;li&gt;The other variable, denoted y, is regarded as the dependent variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Formula&lt;/strong&gt;: $$y = b_0 + b_1 * X_1$$&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://jwlbjtu.github.io/categories/Machine-Learning/"/>
    
      <category term="Regression" scheme="https://jwlbjtu.github.io/categories/Machine-Learning/Regression/"/>
    
    
      <category term="Machine Learning" scheme="https://jwlbjtu.github.io/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="https://jwlbjtu.github.io/tags/Python/"/>
    
      <category term="Scikit-Learn" scheme="https://jwlbjtu.github.io/tags/Scikit-Learn/"/>
    
  </entry>
  
  <entry>
    <title>Data Preprocessing with Python</title>
    <link href="https://jwlbjtu.github.io/2018/01/29/Data-Preprocessing/"/>
    <id>https://jwlbjtu.github.io/2018/01/29/Data-Preprocessing/</id>
    <published>2018-01-30T02:50:41.000Z</published>
    <updated>2018-03-14T23:25:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Data preprocessing is one of the most critical steps before feeding data to machine learning models. A good data preprocessing can greatly improve the performence of the models. One another hand, if data is not prepared properly then the result of any model could be just “Garbage in Garbage out”.<br><a id="more"></a><br>Below are the typical steps to process a dataset:</p><ol><li>Load the dataset, in order to get a sense of the data</li><li>Taking care of missing data (Optional)</li><li>Encoding categorical data (Optional)</li><li>Splitting dataset into the Training set and Test set (Validation set)</li><li>Feature Scaling</li></ol><p>Thanks for all the powerful libararies, today we can implement above steps very easily with Python. </p><h2 id="Import-the-libararies"><a href="#Import-the-libararies" class="headerlink" title="Import the libararies"></a>Import the libararies</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><ul><li><strong><a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a></strong> is a popular libaray for sintific computing. Here will mainly use it’s N-dimensional array object. It also has very useful linear algebra, Fourier transform, and random number capabilities</li><li><strong><a href="https://matplotlib.org/" target="_blank" rel="noopener">matploylib</a></strong> is a Python 2D plotting library which can help us to visulize the dataset </li><li><strong><a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a></strong> is a easy-to-use data structures and data analysis tools for Python. We use it to load and separat datasets.</li><li><strong><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a></strong> is another libaray we will use later. It is a very powerful tool for data analysis. Due to its comprehensive tools we will introduce them indivdualy once we use them. </li></ul><h2 id="Import-the-dataset"><a href="#Import-the-dataset" class="headerlink" title="Import the dataset"></a>Import the dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read a csv file by pandas</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">'Data.csv'</span>)</span><br><span class="line"><span class="comment"># print out the loaded dataset</span></span><br><span class="line">dataset</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th style="text-align:center">Country</th><th style="text-align:center">Age</th><th style="text-align:center">Salary</th><th style="text-align:right">Purchased</th></tr></thead><tbody><tr><td>0</td><td style="text-align:center">France</td><td style="text-align:center">44.0</td><td style="text-align:center">72000.0</td><td style="text-align:right">No</td></tr><tr><td>1</td><td style="text-align:center">Spain</td><td style="text-align:center">27.0</td><td style="text-align:center">48000.0</td><td style="text-align:right">Yes</td></tr><tr><td>2</td><td style="text-align:center">Germany</td><td style="text-align:center">30.0</td><td style="text-align:center">54000.0</td><td style="text-align:right">No</td></tr><tr><td>3</td><td style="text-align:center">Spain</td><td style="text-align:center">38.0</td><td style="text-align:center">61000.0</td><td style="text-align:right">No</td></tr><tr><td>4</td><td style="text-align:center">Germany</td><td style="text-align:center">40.0</td><td style="text-align:center">NaN</td><td style="text-align:right">Yes</td></tr><tr><td>5</td><td style="text-align:center">France</td><td style="text-align:center">35.0</td><td style="text-align:center">58000.0</td><td style="text-align:right">Yes</td></tr><tr><td>6</td><td style="text-align:center">Spain</td><td style="text-align:center">NaN</td><td style="text-align:center">52000.0</td><td style="text-align:right">No</td></tr><tr><td>7</td><td style="text-align:center">France</td><td style="text-align:center">48.0</td><td style="text-align:center">79000.0</td><td style="text-align:right">Yes</td></tr><tr><td>8</td><td style="text-align:center">Germany</td><td style="text-align:center">50.0</td><td style="text-align:center">83000.0</td><td style="text-align:right">No</td></tr><tr><td>9</td><td style="text-align:center">France</td><td style="text-align:center">37.0</td><td style="text-align:center">67000.0</td><td style="text-align:right">Yes</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># separate the dataset into X and y</span></span><br><span class="line"><span class="comment"># X is independent variables. Here are columns 'Country', 'Age' and 'Salary'</span></span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values</span><br><span class="line"><span class="comment"># y is dependent variables. Here is column 'Purchased'</span></span><br><span class="line">y = dataset.iloc[:, <span class="number">-1</span>].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># value of X</span></span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure><pre><code>[[&apos;France&apos; 44.0 72000.0] [&apos;Spain&apos; 27.0 48000.0] [&apos;Germany&apos; 30.0 54000.0] [&apos;Spain&apos; 38.0 61000.0] [&apos;Germany&apos; 40.0 nan] [&apos;France&apos; 35.0 58000.0] [&apos;Spain&apos; nan 52000.0] [&apos;France&apos; 48.0 79000.0] [&apos;Germany&apos; 50.0 83000.0] [&apos;France&apos; 37.0 67000.0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># value of y</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>[&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;]</code></pre><h2 id="Taking-care-of-missing-data"><a href="#Taking-care-of-missing-data" class="headerlink" title="Taking care of missing data"></a>Taking care of missing data</h2><p>If you look closely there are two missing values in the dataset. One is the age of customer 6. Another is the salary of customer 4. Most of the time we need to fullfill the missing values to make the model work. There are three main ways to do it. Using the ‘mean’, ‘median’ or ‘most frequent’. Here I will implement by using the meam of each value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the sklearn libarary</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="comment"># Instanciate the Imputer class</span></span><br><span class="line"><span class="comment"># misstion_value: the place holder for the missting value, here use the default 'NaN'</span></span><br><span class="line"><span class="comment"># strategy: 'mean', 'median' and 'most_frequent'</span></span><br><span class="line"><span class="comment"># aixs: 0 - impute along columns. 1 - impute along rows.</span></span><br><span class="line">imputer = Imputer(missing_values = <span class="string">'NaN'</span>, strategy = <span class="string">'mean'</span>, axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># Fit column Age and Salary</span></span><br><span class="line">imputer = imputer.fit(X[:, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[:, <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[:, <span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># value of X</span></span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure><pre><code>[[&apos;France&apos; 44.0 72000.0] [&apos;Spain&apos; 27.0 48000.0] [&apos;Germany&apos; 30.0 54000.0] [&apos;Spain&apos; 38.0 61000.0] [&apos;Germany&apos; 40.0 63777.77777777778] [&apos;France&apos; 35.0 58000.0] [&apos;Spain&apos; 38.77777777777778 52000.0] [&apos;France&apos; 48.0 79000.0] [&apos;Germany&apos; 50.0 83000.0] [&apos;France&apos; 37.0 67000.0]]</code></pre><p>Here we used sklearn’s Imputer module to help us to take care of the missing data. From the code you can see it become very easy by using the libaray. And the missing Age and Salary are filled with the mean value of their column.</p><h2 id="Encoding-Categorical-Data"><a href="#Encoding-Categorical-Data" class="headerlink" title="Encoding Categorical Data"></a>Encoding Categorical Data</h2><p>In our dataset, the first column is contry name. The values in this column are text not numbers. But the machine learning model only work with numbers. So we need to encode the country name into numbers. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import LabelEncoder to encode text into numbers</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="comment"># Encode first column of X</span></span><br><span class="line">labelEncoder_X = LabelEncoder()</span><br><span class="line">X[:, <span class="number">0</span>] = labelEncoder_X.fit_transform(X[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># print value of X</span></span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure><pre><code>[[0 44.0 72000.0] [2 27.0 48000.0] [1 30.0 54000.0] [2 38.0 61000.0] [1 40.0 63777.77777777778] [0 35.0 58000.0] [2 38.77777777777778 52000.0] [0 48.0 79000.0] [1 50.0 83000.0] [0 37.0 67000.0]]</code></pre><p>After using LabelEncoder, you can see we encode the country names from text into numbers. Here we got ‘France’ -&gt; 0, ‘Germany’ -&gt; 1, ‘Spain’ -&gt; 2. All good, right? NO! Here’s the problem, by encoding ‘France’, ‘Germany’ and ‘Spain’ into 0, 1 and 2, it means ‘Spain’ is greater than ‘Germany’, and ‘Germany’ is greater than ‘France’, just like 2 &gt; 1 &gt;0. This is wrong and all the countries should be considered on the same level. So we need to do some extra work to correct this result. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import OneHotEncoder module</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="comment"># Instantiate OneHotEncoder and set the first column </span></span><br><span class="line">oneHotEncoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Encoder the first column of X and return an array object</span></span><br><span class="line">X = oneHotEncoder.fit_transform(X).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print value of X</span></span><br><span class="line">float_formatter = <span class="keyword">lambda</span> x: <span class="string">"%.2f"</span> % x</span><br><span class="line">np.set_printoptions(formatter = &#123;<span class="string">'float_kind'</span> : float_formatter&#125;)</span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure><pre><code>[[1.00 0.00 0.00 44.00 72000.00] [0.00 0.00 1.00 27.00 48000.00] [0.00 1.00 0.00 30.00 54000.00] [0.00 0.00 1.00 38.00 61000.00] [0.00 1.00 0.00 40.00 63777.78] [1.00 0.00 0.00 35.00 58000.00] [0.00 0.00 1.00 38.78 52000.00] [1.00 0.00 0.00 48.00 79000.00] [0.00 1.00 0.00 50.00 83000.00] [1.00 0.00 0.00 37.00 67000.00]]</code></pre><p>After the categorical encoding, the first column of X is separated into three columns. Each column represent one country. By doing this the model will know which country the customer comes from and make sure the model treat all the country at the same level. Now let’s encode y as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before encoding</span></span><br><span class="line">print(<span class="string">'Before encoding: '</span>)</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since y only has two values, 'yes' and 'no', we can just simply encode the value to 1 and 0 </span></span><br><span class="line">labelEncoder_y = LabelEncoder()</span><br><span class="line">y = labelEncoder_y.fit_transform(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print y</span></span><br><span class="line">print(<span class="string">'After encoding: '</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>Before encoding: [&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;]After encoding: [0 1 0 0 1 1 0 1 0 1]</code></pre><h2 id="Splitting-the-data-set-into-the-Training-set-and-Test-set"><a href="#Splitting-the-data-set-into-the-Training-set-and-Test-set" class="headerlink" title="Splitting the data set into the Training set and Test set"></a>Splitting the data set into the Training set and Test set</h2><p>One thing that every machine learning process will do is to split dataset into Training set and Test set. Just like human, if the machine keep learning the same dataset, it could “learning it by heart”. Which means that the model could perform very accurate prediction on the same dataset, but when given a new dataset it model just performs poorly. We call this kind of scenario “overfitting”. As a result, to avoid this situation we’d like to separate the dataset into Training set which will be used for training the model. And Test set, which test if the model’s performance. If the model performs poorly on the Test set then we can try to correct the settings in the model and retry it. In some cases, people even break the dataset into three portions, training set, test set and validation set. So after trained and tested, validation set can be used to verify the final performance of the model. And this method to validate the model called cross validation.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import 'train_test_split', a very self explaining module</span></span><br><span class="line"><span class="comment"># Please notice the libaray we use is 'model_selection' from 'sklearn'</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># Split X into X_train and X_test. Split y into y_train and y_test</span></span><br><span class="line"><span class="comment"># test_size: usually choose less than 0.5 of the full dataset</span></span><br><span class="line"><span class="comment"># random_state: when splitting the dataset we want make the dataset to a random order first</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out values</span></span><br><span class="line">print(<span class="string">'X_train: '</span>)</span><br><span class="line">print(X_train)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'X_test: '</span>)</span><br><span class="line">print(X_test)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'y_train: '</span>)</span><br><span class="line">print(y_train)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'y_test: '</span>)</span><br><span class="line">print(y_test)</span><br></pre></td></tr></table></figure><pre><code>X_train: [[0.00 1.00 0.00 40.00 63777.78] [1.00 0.00 0.00 37.00 67000.00] [0.00 0.00 1.00 27.00 48000.00] [0.00 0.00 1.00 38.78 52000.00] [1.00 0.00 0.00 48.00 79000.00] [0.00 0.00 1.00 38.00 61000.00] [1.00 0.00 0.00 44.00 72000.00] [1.00 0.00 0.00 35.00 58000.00]]X_test: [[0.00 1.00 0.00 30.00 54000.00] [0.00 1.00 0.00 50.00 83000.00]]y_train: [1 1 1 0 1 0 0 1]y_test: [0 0]</code></pre><h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p>Feature scaling is another must do step for most of the data preprocessing. What it dose is to scale the values in a dataset into a range of -1 ~ 1. To do this has two benefits, firt the computation time is shorter with small scale numbers. Second, is to avoid a certain feature dominate the result due to a bigger scale. For example, in our dataset there are Age and Salary features. The values of Salary are much bigger than Age, so during the training process Salary feature has a chance to dominate the result and make Age feature become useless. So by scale them into the same level, we can avoid this problem to happen.</p><p>There are two main ways to scale the feature: Standardization and Normalization.</p><p>Standardization: (sd -&gt; Standard Deviation) $$x’ = \frac{x - mean(x)}{sd(x)}$$</p><p>Normalization : $$x’ = \frac{x - mean(x)}{max(x) - min(x)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here I use Standardization</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train[:, <span class="number">3</span>:<span class="number">5</span>] = sc_X.fit_transform(X_train[:, <span class="number">3</span>:<span class="number">5</span>])</span><br><span class="line">X_test[:, <span class="number">3</span>:<span class="number">5</span>] = sc_X.transform(X_test[:, <span class="number">3</span>:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out X_train, X_test</span></span><br><span class="line">print(<span class="string">'X_train: '</span>)</span><br><span class="line">print(X_train)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'X_test: '</span>)</span><br><span class="line">print(X_test)</span><br></pre></td></tr></table></figure><pre><code>X_train: [[0.00 1.00 0.00 0.26 0.12] [1.00 0.00 0.00 -0.25 0.46] [0.00 0.00 1.00 -1.98 -1.53] [0.00 0.00 1.00 0.05 -1.11] [1.00 0.00 0.00 1.64 1.72] [0.00 0.00 1.00 -0.08 -0.17] [1.00 0.00 0.00 0.95 0.99] [1.00 0.00 0.00 -0.60 -0.48]]X_test: [[0.00 1.00 0.00 -1.46 -0.90] [0.00 1.00 0.00 1.98 2.14]]</code></pre><p>Now we have all the step implemented for data preprocessing. In practice, not all the steps are needed. Please select the required steps based on your dataset. Below is the full version of the code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data Preprocessing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import the libararies</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import the dataset</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">'Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">-1</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Taking care of missing data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">'NaN'</span>, strategy = <span class="string">'mean'</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[:, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[:, <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[:, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoding Categorical Data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelEncoder_X = LabelEncoder()</span><br><span class="line">X[:, <span class="number">0</span>] = labelEncoder_X.fit_transform(X[:, <span class="number">0</span>])</span><br><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelEncoder_y = LabelEncoder()</span><br><span class="line">y = labelEncoder_y.fit_transform(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Splitting the dataset into the Training set and Test set</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feature Scaling</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.transform(X_test)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Data preprocessing is one of the most critical steps before feeding data to machine learning models. A good data preprocessing can greatly improve the performence of the models. One another hand, if data is not prepared properly then the result of any model could be just “Garbage in Garbage out”.&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://jwlbjtu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Python" scheme="https://jwlbjtu.github.io/tags/Python/"/>
    
      <category term="Machine Leaning" scheme="https://jwlbjtu.github.io/tags/Machine-Leaning/"/>
    
  </entry>
  
  <entry>
    <title>Get back on blogging</title>
    <link href="https://jwlbjtu.github.io/2018/01/13/Get-back-on-blogging/"/>
    <id>https://jwlbjtu.github.io/2018/01/13/Get-back-on-blogging/</id>
    <published>2018-01-13T21:26:50.000Z</published>
    <updated>2018-01-30T06:17:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>It has been a while since I posted my last blog. Today I finally find sometime setting up this new blog site that has a nice looking theme. Now I am feeling so existed to start the new blogging journey!! </p><p>As the first post, let’s talk a little about how I create this blog site.<br><a id="more"></a></p><h3 id="Hexo-Blog-Framework"><a href="#Hexo-Blog-Framework" class="headerlink" title="Hexo - Blog Framework"></a>Hexo - Blog Framework</h3><p><a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> is a NodeJS based blog framework. The set up is very easy, the only two things you need are NodeJS and Git. Once you have it installed and initialized, you can start blogging right away. Hexo is a complete package by itself. </p><p>All the posts are writen by MarkDown (*.md) files, and later Hexo will traslate them into static HTML files during the process of pulishing. The vanilla Hexo already has Post, Archive, Categorize and Tags fundctions. By installing themes on Hexo you could also get Searching and Comment widgets. Hexo also has plugins like Google Analytics, Swiftype, Disqus e.g. So you would get anything you need to start a blog with Hexo. </p><p>The best thing is all the plugins and widgets above are easyly configured in one _config.yml file. No code writing at all! </p><p><strong>Useful Links</strong></p><ul><li><a href="https://hexo.io/docs/" target="_blank" rel="noopener">Hexo Doc</a></li><li><a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo Theme</a></li></ul><h3 id="Hosting-the-blog-on-GitHub-Page"><a href="#Hosting-the-blog-on-GitHub-Page" class="headerlink" title="Hosting the blog on GitHub Page"></a>Hosting the blog on GitHub Page</h3><p>I hosted my blog site by using GutHut Pages. It is a free and easy way to host a web application. Hexo has a deploy plugin for automatically deploy the blog onto GitHub. Details please check out the deployment section of Hexo Doc. Below are the links I found are very helpful:</p><ul><li><a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a></li><li><a href="https://zirho.github.io/2016/06/04/hexo/" target="_blank" rel="noopener">How to setup a blog on github with Hexo</a></li><li><a href="https://help.github.com/articles/quick-start-setting-up-a-custom-domain/" target="_blank" rel="noopener">Setting up a custom domain</a></li></ul><p><strong>Tips:</strong> GitHub Pages requires you to create a repository with name like {username}.github.io, username is your GitHub username. Once you have the repositoy created, please <strong>DO NOT</strong> try to push all the Hexo source code into that repo. How Hexo works is that it will generate static web content based on the local source code and then push only the static content to the repo. And GitHub will scan the repo and set up a host for it. </p><p>So after you create the repository please leave it empty, and then follow the <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">document</a> to configue the _config.yml file. After that just execute the command below Hexo will handle everything for you. After the command is done you should see that static contents are pushed into your repository.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></p><hr><p>Here is the link to my old blogs <a href="https://jiangwl.wordpress.com/2013/12/09/interact-with-mongodb-by-java/" target="_blank" rel="noopener">Old blogs</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It has been a while since I posted my last blog. Today I finally find sometime setting up this new blog site that has a nice looking theme. Now I am feeling so existed to start the new blogging journey!! &lt;/p&gt;
&lt;p&gt;As the first post, let’s talk a little about how I create this blog site.&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="https://jwlbjtu.github.io/categories/Life/"/>
    
    
      <category term="other" scheme="https://jwlbjtu.github.io/tags/other/"/>
    
  </entry>
  
</feed>
