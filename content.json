{"meta":{"title":"Code with Love","subtitle":"The road to learn","description":"Eay Code Love","author":"Wenlong Jiang","url":"https://jwlbjtu.github.io"},"pages":[{"title":"all-categories","date":"2018-01-30T04:26:24.000Z","updated":"2018-01-30T04:26:24.000Z","comments":false,"path":"all-categories/index.html","permalink":"https://jwlbjtu.github.io/all-categories/index.html","excerpt":"","text":""},{"title":"all-archives","date":"2018-01-30T04:28:02.000Z","updated":"2018-01-30T04:28:02.000Z","comments":false,"path":"all-archives/index.html","permalink":"https://jwlbjtu.github.io/all-archives/index.html","excerpt":"","text":""},{"title":"all-tags","date":"2018-01-30T04:27:21.000Z","updated":"2018-01-30T04:27:21.000Z","comments":false,"path":"all-tags/index.html","permalink":"https://jwlbjtu.github.io/all-tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-01-13T19:31:19.000Z","updated":"2018-01-13T19:31:19.000Z","comments":true,"path":"categories/index.html","permalink":"https://jwlbjtu.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-01-13T19:31:23.000Z","updated":"2018-01-13T19:31:23.000Z","comments":true,"path":"tags/index.html","permalink":"https://jwlbjtu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Numpy Cheat Sheet","slug":"Numpy-Cheat-Sheet","date":"2018-03-18T02:00:17.000Z","updated":"2018-03-18T02:03:24.000Z","comments":true,"path":"2018/03/17/Numpy-Cheat-Sheet/","link":"","permalink":"https://jwlbjtu.github.io/2018/03/17/Numpy-Cheat-Sheet/","excerpt":"","text":"Numpy is a popular Python libaray in Machine Learning area. Here summarised some useful tips for Numpy. 1import numpy as np Basics of Numpy Array12345678# creating numpy arrayarray = np.array([[1, 2, 3], [2, 3, 4]])print(array)print('Number of dim:', array.ndim)print('Shape:', array.shape)print('size:', array.size) [[1 2 3] [2 3 4]] Number of dim: 2 Shape: (2, 3) size: 6 123456789101112# define element type of an arrayarray = np.array([1, 2, 3], dtype = np.int)print(array.dtype) # int64array = np.array([1, 2, 3], dtype = np.int32)print(array.dtype) # int32array = np.array([1, 2, 3], dtype = np.float)print(array.dtype) # float64array = np.array([1, 2, 3], dtype = np.float32)print(array.dtype) # float32 int64 int32 float64 float32 Creating Matrix12345678910111213141516171819# creating a matrix with all 0matrix = np.zeros((2,3))print('Zeros matrix')print(matrix)# creating a matrix with all 1matrix = np.ones((3,4))print('Ones matrix')print(matrix)# creating empty matrixmatrix = np.empty((2,2))print('Empty matrix')print(matrix)# creating random matrixmatrix = np.random.random((2,4))print('Random matrix')print(matrix) Zeros matrix [[ 0. 0. 0.] [ 0. 0. 0.]] Ones matrix [[ 1. 1. 1. 1.] [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]] Empty matrix [[ 2.68156159e+154 -2.32036126e+077] [ 6.94773593e-310 2.78136381e-309]] Random matrix [[ 0.3361672 0.15099262 0.43580346 0.07635681] [ 0.21574756 0.40070359 0.64789344 0.66923312]] Numpy Arange123# creating continuous arraya = np.arange(10, 20, 2)print(a) [10 12 14 16 18] 123# creating continuous matrixa = np.arange(12).reshape((3,4))print(a) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 12345678# creating a linspacea = np.linspace(1, 10, 5)print(a)print()a = np.linspace(1, 10, 6).reshape((2, 3))print(a) [ 1. 3.25 5.5 7.75 10. ] [[ 1. 2.8 4.6] [ 6.4 8.2 10. ]] Numpy Operations1234567891011121314151617181920212223# array operationsa = np.array([10, 20, 30, 40])b = np.arange(4)print('a = ', a)print('b = ', b)c = a - bprint('Substraction:', c)c = a + bprint('Sum:', c)c = b**2print('Square:', c)c = 10 * np.sin(a)print('Sin:', c)c = 10 * np.cos(a)print('Cos:', c)c = 10 * np.tan(a)print('Tan:', c) a = [10 20 30 40] b = [0 1 2 3] Substraction: [10 19 28 37] Sum: [10 21 32 43] Square: [0 1 4 9] Sin: [-5.44021111 9.12945251 -9.88031624 7.4511316 ] Cos: [-8.39071529 4.08082062 1.5425145 -6.66938062] Tan: [ 6.48360827 22.37160944 -64.05331197 -11.17214931] 123# array element examinationprint(b)print(b &lt; 3) [0 1 2 3] [ True True True False] 123456789101112131415161718192021# matrix operationa = np.array([[1, 1], [0, 1]])b = np.arange(4).reshape((2,2))print('a = ', a)print('b = ', b)# element multiplyc = a * b print('Element Multiply:')print(c)# matrix multiplyc = np.dot(a, b)print('Matrix Multiply:')print(c)c = a.dot(b)print('Matrix Multiply:')print(c) a = [[1 1] [0 1]] b = [[0 1] [2 3]] Element Multiply: [[0 1] [0 3]] Matrix Multiply: [[2 4] [2 3]] Matrix Multiply: [[2 4] [2 3]] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# matrix internal operationsa = np.random.random((2,4))print('a = ', a)# mean of the matrixprint('mean:', np.mean(a)) print('mean:', a.mean())# average of the matrixprint('average:', np.average(a))# median of the matrixprint('median:', np.median(a)) # sum of the matrixprint('sum:', np.sum(a)) # min element of the matrixprint('min:', np.min(a))# max element of the matrixprint('max:', np.max(a))# sum of each row in the matrixprint('sum of row:', np.sum(a, axis = 1)) # min element of each column in the matrixprint('min of column:', np.min(a, axis = 0))# max element of each row in the matrixprint('max of row:', np.max(a, axis =1))# cumulative sumprint('cumsum:', np.cumsum(a))# diff between elementsprint('diff:', np.diff(a))# sort the matrixprint('sort:', np.sort(a))# matrix transposeprint('trnaspose:')print(np.transpose(a))print('trnaspose:')print(a.T)# flatten a matrixprint('flatten matrix:', a.flatten()) a = [[ 0.81709816 0.84494671 0.48110455 0.15578455] [ 0.26020247 0.20752249 0.93577815 0.10599093]] mean: 0.476053500013 mean: 0.476053500013 average: 0.476053500013 median: 0.370653509283 sum: 3.8084280001 min: 0.10599093105 max: 0.935778148161 sum of row: [ 2.29893396 1.50949404] min of column: [ 0.26020247 0.20752249 0.48110455 0.10599093] max of row: [ 0.84494671 0.93577815] cumsum: [ 0.81709816 1.66204487 2.14314942 2.29893396 2.55913643 2.76665892 3.70243707 3.808428 ] diff: [[ 0.02784855 -0.36384216 -0.32532 ] [-0.05267998 0.72825566 -0.82978722]] sort: [[ 0.15578455 0.48110455 0.81709816 0.84494671] [ 0.10599093 0.20752249 0.26020247 0.93577815]] trnaspose: [[ 0.81709816 0.26020247] [ 0.84494671 0.20752249] [ 0.48110455 0.93577815] [ 0.15578455 0.10599093]] trnaspose: [[ 0.81709816 0.26020247] [ 0.84494671 0.20752249] [ 0.48110455 0.93577815] [ 0.15578455 0.10599093]] flatten matrix: [ 0.81709816 0.84494671 0.48110455 0.15578455 0.26020247 0.20752249 0.93577815 0.10599093] 1234567891011121314151617# indexing in matrixA = np.arange(2, 14).reshape((3, 4))print('A = ')print(A)# min indexprint('min index', np.argmin(A))# max indexprint('max index', np.argmax(A))# find non-zero indexprint('non-zero', np.nonzero(A))# clipprint('clip')print(np.clip(A, 5, 9)) A = [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] min index 0 max index 11 non-zero (array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3])) clip [[5 5 5 5] [6 7 8 9] [9 9 9 9]] Numpy Index1234567# array indexA = np.arange(3, 15)print('A = ')print(A)# find element with indexprint('Element with index 3:', A[3]) A = [ 3 4 5 6 7 8 9 10 11 12 13 14] Element with index 3: 6 12345678910111213141516# matrix indexA = np.arange(3, 15).reshape((3, 4))print('A = ')print(A)# find rowprint('Row with index 2:', A[2])print('Row with index 2:', A[2, :])print('Elment index between 1 and 3(exclude) in row with index 1:', A[1, 1:3])# find column elementsprint('Column with index 2:', A[:, 2])# find elementprint('Element at row index 2 nad column index 1:', A[2][1])print('Element at row index 2 nad column index 1:', A[2, 1]) A = [[ 3 4 5 6] [ 7 8 9 10] [11 12 13 14]] Row with index 2: [11 12 13 14] Row with index 2: [11 12 13 14] Elment index between 1 and 3(exclude) in row with index 1: [8 9] Column with index 2: [ 5 9 13] Element at row index 2 nad column index 1: 12 Element at row index 2 nad column index 1: 12 Iterate Matrix12345# iteration of matrixi = 0for row in A: print('row no.', i, row) i += 1 row no. 0 [3 4 5 6] row no. 1 [ 7 8 9 10] row no. 2 [11 12 13 14] 1234i = 0for column in A.T: print('colomun no.', i, column) i += 1 colomun no. 0 [ 3 7 11] colomun no. 1 [ 4 8 12] colomun no. 2 [ 5 9 13] colomun no. 3 [ 6 10 14] 12for item in A.flat: print(item) 3 4 5 6 7 8 9 10 11 12 13 14 Merage123456789101112131415161718192021222324A = np.array([1, 1, 1])B = np.array([2, 2, 2])print('A =', A)print('B =', B)# vertical stackprint('vertical stack:')print(np.vstack((A, B)))# horizontal stackprint('horizontal stack:')print(np.hstack((A, B)))# add dimensionprint('add row dimension:', A[np.newaxis, :])print('add column dimension:')print(A[:, np.newaxis])# concatenateA = A[:, np.newaxis]B = B[:, np.newaxis]C = np.concatenate((A, B, B, A), axis = 1)print('concatenate:')print(C) A = [1 1 1] B = [2 2 2] vertical stack: [[1 1 1] [2 2 2]] horizontal stack: [1 1 1 2 2 2] add row dimension: [[1 1 1]] add column dimension: [[1] [1] [1]] concatenate: [[1 2 2 1] [1 2 2 1] [1 2 2 1]] Divide1234567891011121314151617A = np.arange(12).reshape((3, 4))print('A =')print(A)# horizontal divideprint('horizontal divide')print(np.split(A, 2, axis = 1))print(np.hsplit(A, 2))# vertical divideprint('vertical divide')print(np.split(A, 3, axis = 0))print(np.vsplit(A, 3))# non-even divideprint('non-even divide')print(np.array_split(A, 3, axis = 1)) A = [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] horizontal divide [array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])] [array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])] vertical divide [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] non-even divide [array([[0, 1], [4, 5], [8, 9]]), array([[ 2], [ 6], [10]]), array([[ 3], [ 7], [11]])] Copy12345678910a = np.arange(4)print('a:', a)b = a c = np.copy(a)a[1:3] = [100, 111]print('a:', a)print('b:', b)print('c:', c) a: [0 1 2 3] a: [ 0 100 111 3] b: [ 0 100 111 3] c: [0 1 2 3]","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jwlbjtu.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jwlbjtu.github.io/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"https://jwlbjtu.github.io/tags/Numpy/"}]},{"title":"Multiple Linear Regression by Scikit-Learn","slug":"Multiple-Linear-Regression-by-Scikit-Learn","date":"2018-03-16T03:22:31.000Z","updated":"2018-03-16T03:53:22.000Z","comments":true,"path":"2018/03/15/Multiple-Linear-Regression-by-Scikit-Learn/","link":"","permalink":"https://jwlbjtu.github.io/2018/03/15/Multiple-Linear-Regression-by-Scikit-Learn/","excerpt":"Multiple Linear Regression IntuitionMultiple Linear Regression presents linear relationship between mutiple independent variables and dependent variable. Formula: $$y = b_0 + b_1 * X_1 + … + b_n * X_n$$","text":"Multiple Linear Regression IntuitionMultiple Linear Regression presents linear relationship between mutiple independent variables and dependent variable. Formula: $$y = b_0 + b_1 * X_1 + … + b_n * X_n$$ Multiple Linear Regression Implementaion12345678910# Importing the libaraiesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd# Importing the datasetdataset = pd.read_csv('50_Startups.csv')# show datasetdataset R&amp;D Spend Administration Marketing Spend State Profit 0 165349.20 136897.80 471784.10 New York 192261.83 1 162597.70 151377.59 443898.53 California 191792.06 2 153441.51 101145.55 407934.54 Florida 191050.39 3 144372.41 118671.85 383199.62 New York 182901.99 4 142107.34 91391.77 366168.42 Florida 166187.94 5 131876.90 99814.71 362861.36 New York 156991.12 6 134615.46 147198.87 127716.82 California 156122.51 7 130298.13 145530.06 323876.68 Florida 155752.60 8 120542.52 148718.95 311613.29 New York 152211.77 9 123334.88 108679.17 304981.62 California 149759.96 … 12X = dataset.iloc[:, :-1].valuesy = dataset.iloc[:, 4].values 12# Show XX array([[165349.2, 136897.8, 471784.1, &apos;New York&apos;], [162597.7, 151377.59, 443898.53, &apos;California&apos;], [153441.51, 101145.55, 407934.54, &apos;Florida&apos;], [144372.41, 118671.85, 383199.62, &apos;New York&apos;], [142107.34, 91391.77, 366168.42, &apos;Florida&apos;], [131876.9, 99814.71, 362861.36, &apos;New York&apos;], [134615.46, 147198.87, 127716.82, &apos;California&apos;], [130298.13, 145530.06, 323876.68, &apos;Florida&apos;], [120542.52, 148718.95, 311613.29, &apos;New York&apos;], [123334.88, 108679.17, 304981.62, &apos;California&apos;], ...], dtype=object) 12# Show yy array([ 192261.83, 191792.06, 191050.39, 182901.99, 166187.94, 156991.12, 156122.51, 155752.6 , 152211.77, 149759.96, ... ]) 123456789# Caegorical data encodingfrom sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelEncoder_X = LabelEncoder()X[:, 3]= labelEncoder_X.fit_transform(X[:, 3])oneHotEncoder = OneHotEncoder(categorical_features = [3])X = oneHotEncoder.fit_transform(X).toarray()# Show XX array([[ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.65349200e+05, 1.36897800e+05, 4.71784100e+05], [ 1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.62597700e+05, 1.51377590e+05, 4.43898530e+05], [ 0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.53441510e+05, 1.01145550e+05, 4.07934540e+05], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.44372410e+05, 1.18671850e+05, 3.83199620e+05], [ 0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.42107340e+05, 9.13917700e+04, 3.66168420e+05], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.31876900e+05, 9.98147100e+04, 3.62861360e+05], [ 1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.34615460e+05, 1.47198870e+05, 1.27716820e+05], [ 0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.30298130e+05, 1.45530060e+05, 3.23876680e+05], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.20542520e+05, 1.48718950e+05, 3.11613290e+05], [ 1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.23334880e+05, 1.08679170e+05, 3.04981620e+05], ...]) During the categorical data encoding, we created three dummy variables to present ‘New York’, ‘California’ and ‘Florida’. Because they have strong co-relationship between each other, to avoid dummy variable trap we need to remove one dummy variable. 12345# Avoiding the Dummy Variable TrapX = X[:, 1:]# Show XX array([[ 0.00000000e+00, 1.00000000e+00, 1.65349200e+05, 1.36897800e+05, 4.71784100e+05], [ 0.00000000e+00, 0.00000000e+00, 1.62597700e+05, 1.51377590e+05, 4.43898530e+05], [ 1.00000000e+00, 0.00000000e+00, 1.53441510e+05, 1.01145550e+05, 4.07934540e+05], [ 0.00000000e+00, 1.00000000e+00, 1.44372410e+05, 1.18671850e+05, 3.83199620e+05], [ 1.00000000e+00, 0.00000000e+00, 1.42107340e+05, 9.13917700e+04, 3.66168420e+05], [ 0.00000000e+00, 1.00000000e+00, 1.31876900e+05, 9.98147100e+04, 3.62861360e+05], [ 0.00000000e+00, 0.00000000e+00, 1.34615460e+05, 1.47198870e+05, 1.27716820e+05], [ 1.00000000e+00, 0.00000000e+00, 1.30298130e+05, 1.45530060e+05, 3.23876680e+05], [ 0.00000000e+00, 1.00000000e+00, 1.20542520e+05, 1.48718950e+05, 3.11613290e+05], [ 0.00000000e+00, 0.00000000e+00, 1.23334880e+05, 1.08679170e+05, 3.04981620e+05], ...]) 1234567891011121314# Split the dataset into Training set and Test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)# Fitting Multiple Linear Regression to the Training setfrom sklearn.linear_model import LinearRegressionregressor = LinearRegression()regressor.fit(X_train, y_train)# Predicting the Test sety_pred = regressor.predict(X_test)# Show predict resulty_pred array([ 103015.20159795, 132582.27760817, 132447.73845176, 71976.09851257, 178537.48221058, 116161.24230165, 67851.69209675, 98791.73374686, 113969.43533013, 167921.06569553]) 12# Show Test set resulty_test array([ 103282.38, 144259.4 , 146121.95, 77798.83, 191050.39, 105008.31, 81229.06, 97483.56, 110352.25, 166187.94])","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jwlbjtu.github.io/categories/Machine-Learning/"},{"name":"Regression","slug":"Machine-Learning/Regression","permalink":"https://jwlbjtu.github.io/categories/Machine-Learning/Regression/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jwlbjtu.github.io/tags/Python/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jwlbjtu.github.io/tags/Machine-Learning/"},{"name":"Scikit-Learn","slug":"Scikit-Learn","permalink":"https://jwlbjtu.github.io/tags/Scikit-Learn/"}]},{"title":"Simple Linear Regression by Scikit-Learn","slug":"Simple-Linear-Regression-by-Scikit-Learn","date":"2018-03-14T22:29:31.000Z","updated":"2018-03-14T23:31:34.000Z","comments":true,"path":"2018/03/14/Simple-Linear-Regression-by-Scikit-Learn/","link":"","permalink":"https://jwlbjtu.github.io/2018/03/14/Simple-Linear-Regression-by-Scikit-Learn/","excerpt":"Simple Linear Regression IntuitionSimple linear regression is a statistical method that allows us to summarize and study relationships between two continuous variables: One variable denoted X, is regarded as independent vaiable. The other variable, denoted y, is regarded as the dependent variable. Formula: $$y = b_0 + b_1 * X_1$$","text":"Simple Linear Regression IntuitionSimple linear regression is a statistical method that allows us to summarize and study relationships between two continuous variables: One variable denoted X, is regarded as independent vaiable. The other variable, denoted y, is regarded as the dependent variable. Formula: $$y = b_0 + b_1 * X_1$$ Simple Linear Regression ImplimentationHere we use a simple dataset to train a simple linear regression model. The dataset contains the year of working experences and the salary for each working years. 123456789# Importing the Libaraiesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd# Importing the Datasetdataset = pd.read_csv('Salary_Data.csv')X = dataset.iloc[:, :-1].valuesy = dataset.iloc[:, 1].values 12# Show X - The year of working experiencesX array([[ 1.1], [ 1.3], [ 1.5], [ 2. ], [ 2.2], [ 2.9], [ 3. ], [ 3.2], [ 3.2], [ 3.7], [ 3.9], [ 4. ], [ 4. ], [ 4.1], [ 4.5], [ 4.9], [ 5.1], [ 5.3], [ 5.9], [ 6. ], [ 6.8], [ 7.1], [ 7.9], [ 8.2], [ 8.7], [ 9. ], [ 9.5], [ 9.6], [ 10.3], [ 10.5]]) 12# Show y - The salary reflected by the working experiencesy array([ 39343., 46205., 37731., 43525., 39891., 56642., 60150., 54445., 64445., 57189., 63218., 55794., 56957., 57081., 61111., 67938., 66029., 83088., 81363., 93940., 91738., 98273., 101302., 113812., 109431., 105582., 116969., 112635., 122391., 121872.]) 1%matplotlib inline 123456# Plot the datasetplt.scatter(X, y, color = 'red')plt.title('Salary vs Experience')plt.xlabel('Years of Experience')plt.ylabel('Salary')plt.show() From the diagram we can see that he dataset is following some kind of linear relationship between X and y. Our goal is to find a model that could fit the data perfectly. 123456789101112131415# Splitting the dataset into Training set and Test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)# Fitting Simple Linear Regression to the Training set# Scikit-Learn does the feature scaling for us so we do not need to do thatfrom sklearn.linear_model import LinearRegressionregressor = LinearRegression()regressor.fit(X_train, y_train)# Predict the Test set resulty_pred = regressor.predict(X_test)# Print y_predy_pred array([ 40835.10590871, 123079.39940819, 65134.55626083, 63265.36777221, 115602.64545369, 108125.8914992 , 116537.23969801, 64199.96201652, 76349.68719258, 100649.1375447 ]) 12# Compare with y_testy_test array([ 37731., 122391., 57081., 63218., 116969., 109431., 112635., 55794., 83088., 101302.]) 1234567# Viualising the Training set resultplt.scatter(X_train, y_train, color = 'red')plt.plot(X_train, regressor.predict(X_train), color = 'blue')plt.title('Salary vs Experience')plt.xlabel('Years of Experience')plt.ylabel('Salary')plt.show() 1234567# Viualising the Test set resultplt.scatter(X_test, y_test, color = 'red')plt.plot(X_train, regressor.predict(X_train), color = 'blue')plt.title('Salary vs Experience')plt.xlabel('Years of Experience')plt.ylabel('Salary')plt.show() By using Scikit-Learn LinearRegression module, we trained out model with X_train, y_train. And we found our Simple Linear Regression model showing as the blue line in above two diagrams. In the last diagram we can see our model fits the test dataset very well.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jwlbjtu.github.io/categories/Machine-Learning/"},{"name":"Regression","slug":"Machine-Learning/Regression","permalink":"https://jwlbjtu.github.io/categories/Machine-Learning/Regression/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jwlbjtu.github.io/tags/Python/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jwlbjtu.github.io/tags/Machine-Learning/"},{"name":"Scikit-Learn","slug":"Scikit-Learn","permalink":"https://jwlbjtu.github.io/tags/Scikit-Learn/"}]},{"title":"Data Preprocessing with Python","slug":"Data-Preprocessing","date":"2018-01-30T02:50:41.000Z","updated":"2018-03-14T23:25:14.000Z","comments":true,"path":"2018/01/29/Data-Preprocessing/","link":"","permalink":"https://jwlbjtu.github.io/2018/01/29/Data-Preprocessing/","excerpt":"Data preprocessing is one of the most critical steps before feeding data to machine learning models. A good data preprocessing can greatly improve the performence of the models. One another hand, if data is not prepared properly then the result of any model could be just “Garbage in Garbage out”.","text":"Data preprocessing is one of the most critical steps before feeding data to machine learning models. A good data preprocessing can greatly improve the performence of the models. One another hand, if data is not prepared properly then the result of any model could be just “Garbage in Garbage out”.Below are the typical steps to process a dataset: Load the dataset, in order to get a sense of the data Taking care of missing data (Optional) Encoding categorical data (Optional) Splitting dataset into the Training set and Test set (Validation set) Feature Scaling Thanks for all the powerful libararies, today we can implement above steps very easily with Python. Import the libararies123import numpy as npimport matplotlib.pyplot as pltimport pandas as pd numpy is a popular libaray for sintific computing. Here will mainly use it’s N-dimensional array object. It also has very useful linear algebra, Fourier transform, and random number capabilities matploylib is a Python 2D plotting library which can help us to visulize the dataset pandas is a easy-to-use data structures and data analysis tools for Python. We use it to load and separat datasets. sklearn is another libaray we will use later. It is a very powerful tool for data analysis. Due to its comprehensive tools we will introduce them indivdualy once we use them. Import the dataset1234# read a csv file by pandasdataset = pd.read_csv('Data.csv')# print out the loaded datasetdataset Country Age Salary Purchased 0 France 44.0 72000.0 No 1 Spain 27.0 48000.0 Yes 2 Germany 30.0 54000.0 No 3 Spain 38.0 61000.0 No 4 Germany 40.0 NaN Yes 5 France 35.0 58000.0 Yes 6 Spain NaN 52000.0 No 7 France 48.0 79000.0 Yes 8 Germany 50.0 83000.0 No 9 France 37.0 67000.0 Yes 12345# separate the dataset into X and y# X is independent variables. Here are columns 'Country', 'Age' and 'Salary'X = dataset.iloc[:, :-1].values# y is dependent variables. Here is column 'Purchased'y = dataset.iloc[:, -1].values 12# value of Xprint(X) [[&apos;France&apos; 44.0 72000.0] [&apos;Spain&apos; 27.0 48000.0] [&apos;Germany&apos; 30.0 54000.0] [&apos;Spain&apos; 38.0 61000.0] [&apos;Germany&apos; 40.0 nan] [&apos;France&apos; 35.0 58000.0] [&apos;Spain&apos; nan 52000.0] [&apos;France&apos; 48.0 79000.0] [&apos;Germany&apos; 50.0 83000.0] [&apos;France&apos; 37.0 67000.0]] 12# value of yprint(y) [&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;] Taking care of missing dataIf you look closely there are two missing values in the dataset. One is the age of customer 6. Another is the salary of customer 4. Most of the time we need to fullfill the missing values to make the model work. There are three main ways to do it. Using the ‘mean’, ‘median’ or ‘most frequent’. Here I will implement by using the meam of each value. 12345678910# Import the sklearn libararyfrom sklearn.preprocessing import Imputer# Instanciate the Imputer class# misstion_value: the place holder for the missting value, here use the default 'NaN'# strategy: 'mean', 'median' and 'most_frequent'# aixs: 0 - impute along columns. 1 - impute along rows.imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)# Fit column Age and Salaryimputer = imputer.fit(X[:, 1:3])X[:, 1:3] = imputer.transform(X[:, 1:3]) 12# value of Xprint(X) [[&apos;France&apos; 44.0 72000.0] [&apos;Spain&apos; 27.0 48000.0] [&apos;Germany&apos; 30.0 54000.0] [&apos;Spain&apos; 38.0 61000.0] [&apos;Germany&apos; 40.0 63777.77777777778] [&apos;France&apos; 35.0 58000.0] [&apos;Spain&apos; 38.77777777777778 52000.0] [&apos;France&apos; 48.0 79000.0] [&apos;Germany&apos; 50.0 83000.0] [&apos;France&apos; 37.0 67000.0]] Here we used sklearn’s Imputer module to help us to take care of the missing data. From the code you can see it become very easy by using the libaray. And the missing Age and Salary are filled with the mean value of their column. Encoding Categorical DataIn our dataset, the first column is contry name. The values in this column are text not numbers. But the machine learning model only work with numbers. So we need to encode the country name into numbers. 12345678# Import LabelEncoder to encode text into numbersfrom sklearn.preprocessing import LabelEncoder# Encode first column of XlabelEncoder_X = LabelEncoder()X[:, 0] = labelEncoder_X.fit_transform(X[:, 0])# print value of Xprint(X) [[0 44.0 72000.0] [2 27.0 48000.0] [1 30.0 54000.0] [2 38.0 61000.0] [1 40.0 63777.77777777778] [0 35.0 58000.0] [2 38.77777777777778 52000.0] [0 48.0 79000.0] [1 50.0 83000.0] [0 37.0 67000.0]] After using LabelEncoder, you can see we encode the country names from text into numbers. Here we got ‘France’ -&gt; 0, ‘Germany’ -&gt; 1, ‘Spain’ -&gt; 2. All good, right? NO! Here’s the problem, by encoding ‘France’, ‘Germany’ and ‘Spain’ into 0, 1 and 2, it means ‘Spain’ is greater than ‘Germany’, and ‘Germany’ is greater than ‘France’, just like 2 &gt; 1 &gt;0. This is wrong and all the countries should be considered on the same level. So we need to do some extra work to correct this result. 1234567891011# Import OneHotEncoder modulefrom sklearn.preprocessing import OneHotEncoder# Instantiate OneHotEncoder and set the first column oneHotEncoder = OneHotEncoder(categorical_features = [0])# Encoder the first column of X and return an array objectX = oneHotEncoder.fit_transform(X).toarray()# print value of Xfloat_formatter = lambda x: \"%.2f\" % xnp.set_printoptions(formatter = &#123;'float_kind' : float_formatter&#125;)print(X) [[1.00 0.00 0.00 44.00 72000.00] [0.00 0.00 1.00 27.00 48000.00] [0.00 1.00 0.00 30.00 54000.00] [0.00 0.00 1.00 38.00 61000.00] [0.00 1.00 0.00 40.00 63777.78] [1.00 0.00 0.00 35.00 58000.00] [0.00 0.00 1.00 38.78 52000.00] [1.00 0.00 0.00 48.00 79000.00] [0.00 1.00 0.00 50.00 83000.00] [1.00 0.00 0.00 37.00 67000.00]] After the categorical encoding, the first column of X is separated into three columns. Each column represent one country. By doing this the model will know which country the customer comes from and make sure the model treat all the country at the same level. Now let’s encode y as well. 1234567891011# before encodingprint('Before encoding: ')print(y)# Since y only has two values, 'yes' and 'no', we can just simply encode the value to 1 and 0 labelEncoder_y = LabelEncoder()y = labelEncoder_y.fit_transform(y)# print yprint('After encoding: ')print(y) Before encoding: [&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;] After encoding: [0 1 0 0 1 1 0 1 0 1] Splitting the data set into the Training set and Test setOne thing that every machine learning process will do is to split dataset into Training set and Test set. Just like human, if the machine keep learning the same dataset, it could “learning it by heart”. Which means that the model could perform very accurate prediction on the same dataset, but when given a new dataset it model just performs poorly. We call this kind of scenario “overfitting”. As a result, to avoid this situation we’d like to separate the dataset into Training set which will be used for training the model. And Test set, which test if the model’s performance. If the model performs poorly on the Test set then we can try to correct the settings in the model and retry it. In some cases, people even break the dataset into three portions, training set, test set and validation set. So after trained and tested, validation set can be used to verify the final performance of the model. And this method to validate the model called cross validation. 1234567891011121314151617181920# Import 'train_test_split', a very self explaining module# Please notice the libaray we use is 'model_selection' from 'sklearn'from sklearn.model_selection import train_test_split# Split X into X_train and X_test. Split y into y_train and y_test# test_size: usually choose less than 0.5 of the full dataset# random_state: when splitting the dataset we want make the dataset to a random order firstX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)# Print out valuesprint('X_train: ')print(X_train)print()print('X_test: ')print(X_test)print()print('y_train: ')print(y_train)print()print('y_test: ')print(y_test) X_train: [[0.00 1.00 0.00 40.00 63777.78] [1.00 0.00 0.00 37.00 67000.00] [0.00 0.00 1.00 27.00 48000.00] [0.00 0.00 1.00 38.78 52000.00] [1.00 0.00 0.00 48.00 79000.00] [0.00 0.00 1.00 38.00 61000.00] [1.00 0.00 0.00 44.00 72000.00] [1.00 0.00 0.00 35.00 58000.00]] X_test: [[0.00 1.00 0.00 30.00 54000.00] [0.00 1.00 0.00 50.00 83000.00]] y_train: [1 1 1 0 1 0 0 1] y_test: [0 0] Feature ScalingFeature scaling is another must do step for most of the data preprocessing. What it dose is to scale the values in a dataset into a range of -1 ~ 1. To do this has two benefits, firt the computation time is shorter with small scale numbers. Second, is to avoid a certain feature dominate the result due to a bigger scale. For example, in our dataset there are Age and Salary features. The values of Salary are much bigger than Age, so during the training process Salary feature has a chance to dominate the result and make Age feature become useless. So by scale them into the same level, we can avoid this problem to happen. There are two main ways to scale the feature: Standardization and Normalization. Standardization: (sd -&gt; Standard Deviation) $$x’ = \\frac{x - mean(x)}{sd(x)}$$ Normalization : $$x’ = \\frac{x - mean(x)}{max(x) - min(x)}$$ 123456789101112# Here I use Standardizationfrom sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train[:, 3:5] = sc_X.fit_transform(X_train[:, 3:5])X_test[:, 3:5] = sc_X.transform(X_test[:, 3:5])# Print out X_train, X_testprint('X_train: ')print(X_train)print()print('X_test: ')print(X_test) X_train: [[0.00 1.00 0.00 0.26 0.12] [1.00 0.00 0.00 -0.25 0.46] [0.00 0.00 1.00 -1.98 -1.53] [0.00 0.00 1.00 0.05 -1.11] [1.00 0.00 0.00 1.64 1.72] [0.00 0.00 1.00 -0.08 -0.17] [1.00 0.00 0.00 0.95 0.99] [1.00 0.00 0.00 -0.60 -0.48]] X_test: [[0.00 1.00 0.00 -1.46 -0.90] [0.00 1.00 0.00 1.98 2.14]] Now we have all the step implemented for data preprocessing. In practice, not all the steps are needed. Please select the required steps based on your dataset. Below is the full version of the code. 123456789101112131415161718192021222324252627282930313233343536# Data Preprocessing# Import the libarariesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd# Import the datasetdataset = pd.read_csv('Data.csv')X = dataset.iloc[:, :-1].valuesy = dataset.iloc[:, -1].values# Taking care of missing datafrom sklearn.preprocessing import Imputerimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)imputer = imputer.fit(X[:, 1:3])X[:, 1:3] = imputer.transform(X[:, 1:3])# Encoding Categorical Datafrom sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelEncoder_X = LabelEncoder()X[:, 0] = labelEncoder_X.fit_transform(X[:, 0])onehotencoder = OneHotEncoder(categorical_features = [0])X = onehotencoder.fit_transform(X).toarray()labelEncoder_y = LabelEncoder()y = labelEncoder_y.fit_transform(y)# Splitting the dataset into the Training set and Test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)# Feature Scalingfrom sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train = sc_X.fit_transform(X_train)X_test = sc_X.transform(X_test)","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jwlbjtu.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Leaning","slug":"Machine-Leaning","permalink":"https://jwlbjtu.github.io/tags/Machine-Leaning/"},{"name":"Python","slug":"Python","permalink":"https://jwlbjtu.github.io/tags/Python/"}]},{"title":"Get back on blogging","slug":"Get-back-on-blogging","date":"2018-01-13T21:26:50.000Z","updated":"2018-01-30T06:17:06.000Z","comments":true,"path":"2018/01/13/Get-back-on-blogging/","link":"","permalink":"https://jwlbjtu.github.io/2018/01/13/Get-back-on-blogging/","excerpt":"It has been a while since I posted my last blog. Today I finally find sometime setting up this new blog site that has a nice looking theme. Now I am feeling so existed to start the new blogging journey!! As the first post, let’s talk a little about how I create this blog site.","text":"It has been a while since I posted my last blog. Today I finally find sometime setting up this new blog site that has a nice looking theme. Now I am feeling so existed to start the new blogging journey!! As the first post, let’s talk a little about how I create this blog site. Hexo - Blog FrameworkHexo is a NodeJS based blog framework. \bThe set up is very easy, the only two things you need are NodeJS and Git. Once you have it installed and initialized, you can start blogging right away. Hexo is a complete package by itself. All the posts are writen by MarkDown (*.md) files, and later Hexo will traslate them into static HTML files during the process of pulishing. The vanilla Hexo already has Post, Archive, Categorize and Tags fundctions. By installing themes on Hexo you could also get Searching and Comment widgets. Hexo also has plugins like Google Analytics, Swiftype, Disqus e.g. So you would get anything you need to start a blog with Hexo. The best thing is all the plugins and widgets above are easyly configured in one _config.yml file. No code writing at all! Useful Links Hexo Doc Hexo Theme Hosting the blog on GitHub PageI hosted my blog site by using GutHut \bPages. It is a free and easy way to host a web application. Hexo has a deploy plugin for automatically deploy the blog onto GitHub. Details please check out the deployment section of Hexo Doc. Below are the links I found are very helpful: GitHub Pages How to setup a blog on github with Hexo Setting up a custom domain Tips: GitHub Pages requires you to create a repository with name like {username}.github.io, username is your GitHub username. Once you have the repositoy created, please DO NOT try to push all the Hexo source code into that repo. How Hexo works is that it will generate static web content based on the local source code and then push only the static content to the repo. And GitHub will scan the repo and set up a host for it. So after you create the repository please leave it empty, and then follow the document to configue the _config.yml file. After that just execute the command below Hexo will handle everything for you. After the command is done you should see that static contents are pushed into your repository.1$ hexo deploy Here is the link to my old blogs Old blogs","categories":[{"name":"Life","slug":"Life","permalink":"https://jwlbjtu.github.io/categories/Life/"}],"tags":[{"name":"other","slug":"other","permalink":"https://jwlbjtu.github.io/tags/other/"}]}]}